{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mq2o5gEMBRzb"
      },
      "source": [
        "Term frequency (TF) and inverse document frequency (IDF) are used to find the impact and importance of a word in a given sentence in natural language processing. TF-IDF, combined with XGBoost, which is a scalable tree boosting ML model, can tell if a loan will default. TF-IDF and XGBoost are a shallower and more light-weighted approach, though they are not as accurate as those of deep learning models (e.g. BERT). Here are the steps to conduct the experiment:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xfsoCBWG97ir"
      },
      "source": [
        "Install and upgrade XGBoost:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-XxL3Pi6ynKe"
      },
      "source": [
        "!pip3 install --upgrade xgboost"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nMBw_mPz-BGj"
      },
      "source": [
        "Import TF-IDF Vectorizer from scikit-learn:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "axpE7SL49ZVv"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1xcjjdyb-RiR"
      },
      "source": [
        "Download Kiva's train and test datasets:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BgCjTFQBnm5f"
      },
      "source": [
        "!wget -O kiva_train.csv https://drive.google.com/u/0/uc?id=1dzzVbgHphbCf7kvq9IKiIhwzmxPbuH4s&export=download\n",
        "!wget -O kiva_test.csv https://drive.google.com/u/0/uc?id=1EVWfyqQOd_W2uTKrr4JTD2iFrEZHoOHT&export=download"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mKaeP_Jt-eO_"
      },
      "source": [
        "Load train and test datasets from CSV files:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zkxCi2CQWOCD"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "train_dataset = pd.read_csv (r'kiva_train.csv')\n",
        "test_dataset = pd.read_csv (r'kiva_test.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gr283dbo-4Ln"
      },
      "source": [
        "Set aside 10% of data from train dataset for validation:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "90tm-joeWbcg"
      },
      "source": [
        "eval_dataset = train_dataset.sample(frac=0.1, random_state=2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6bqNuVIr_CF8"
      },
      "source": [
        "Note: The following block should be removed when the model is used to predict loan defaults of the held-out test dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J9v_R5Ze0aK6"
      },
      "source": [
        "train_set_aside = []\n",
        "for index, row in train_dataset.iterrows():\n",
        "    if row.loan_id not in list(eval_dataset[\"loan_id\"]):\n",
        "        train_set_aside.append(row)\n",
        "train_dataset = pd.DataFrame(train_set_aside)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kRDWlpNhAdzj"
      },
      "source": [
        "Calculate TF-IDF values on all datasets:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "46aSsPiBFk6l"
      },
      "source": [
        "# create object\n",
        "tfidf = TfidfVectorizer()\n",
        "\n",
        "# get tf-df values\n",
        "tfidf_vectorizer = tfidf.fit(list(train_dataset[\"en_clean\"]))\n",
        "train_tfidf_vectors = tfidf_vectorizer.transform(list(train_dataset[\"en_clean\"]))\n",
        "eval_tfidf_vectors = tfidf_vectorizer.transform(list(eval_dataset[\"en_clean\"]))\n",
        "test_tfidf_vectors = tfidf_vectorizer.transform(list(test_dataset[\"en_clean\"]))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7sDGS8nLAvLO"
      },
      "source": [
        "Train the model using XGBoost and TD-IDF features from the previous steps:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OXaasHDPbZO5"
      },
      "source": [
        "import xgboost as xgb\n",
        "train_features = pd.DataFrame(train_tfidf_vectors.toarray())\n",
        "dtrain = xgb.DMatrix(train_features, label=pd.DataFrame(train_dataset[\"defaulted\"]))\n",
        "dtrain.save_binary('train.buffer')\n",
        "\n",
        "eval_features = pd.DataFrame(eval_tfidf_vectors.toarray())\n",
        "deval = xgb.DMatrix(eval_features, label=pd.DataFrame(eval_dataset[\"defaulted\"]))\n",
        "deval.save_binary('eval.buffer')\n",
        "\n",
        "test_features = pd.DataFrame(test_tfidf_vectors.toarray())\n",
        "dtest = xgb.DMatrix(test_features)\n",
        "dtest.save_binary('test.buffer')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Up-xPGu1lOjz"
      },
      "source": [
        "param = {'max_depth': 9, 'eta': 0.1, 'objective': 'binary:logistic'}\n",
        "param['nthread'] = 4\n",
        "\n",
        "param['eval_metric'] = ['error','auc']\n",
        "\n",
        "evallist = [(dtrain, 'train'),(deval, 'eval')]\n",
        "\n",
        "num_round = 41\n",
        "bst = xgb.train(param, dtrain, num_round, evallist, early_stopping_rounds=3)\n",
        "#bst = xgb.train(param, dtrain, num_round, evallist)\n",
        "\n",
        "bst.save_model('0001.model')\n",
        "bst.dump_model('dump.raw.txt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hAOg_XppBG58"
      },
      "source": [
        "Predict loan defaults of the held-out test dataset:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Aiuc9ReBepHG"
      },
      "source": [
        "test_pred = bst.predict(dtest, iteration_range=(0, bst.best_iteration + 1))\n",
        "test_pred = [int(x) for x in (test_pred>0.5)]\n",
        "test_dataset[\"defaulted\"] = test_pred\n",
        "test_dataset.to_csv(\"kiva_test_with_defaulted.csv\",index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dRlUGot3K5Zx"
      },
      "source": [
        "Code for debug: Dump validation predictions and validation labels, and print validation accuracy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "faIK1iWxr5jv"
      },
      "source": [
        "deval2 = xgb.DMatrix(eval_features)\n",
        "eval_pred = bst.predict(deval2, iteration_range=(0, bst.best_iteration + 1))\n",
        "eval_pred = [int(x) for x in (eval_pred>0.5)]\n",
        "print(eval_pred)\n",
        "print(list(eval_dataset[\"defaulted\"]))\n",
        "\n",
        "import sklearn\n",
        "import numpy as np\n",
        "acc = sklearn.metrics.accuracy_score(np.array(eval_pred), np.array(list(eval_dataset[\"defaulted\"])))\n",
        "print(\"Validation Accuracy: \", acc)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}