{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8XpgaR6uy9QG"
      },
      "source": [
        "The BERT machine learning model is a bidirectional transformer pretrained using masked language objective and next sentence prediction. The BERT model have been successfully applied to natural language processing tasks such as question answering, sentiment analysis, and document summarization, just name a few. In order to evaluate if BERT can predict loan defaults specifically, there are a few steps to go:\n",
        "\n",
        "\n",
        "1.   Install TensorFlow 2.x.\n",
        "2.   Download Kiva's train and test dataset.\n",
        "3.   Load train and test dataset from CSV files.\n",
        "4.   Tokenize English texts in both datasets using the BERT tokenizer.\n",
        "5.   Load the pretrained BERT model from huggingface.\n",
        "6.   Fine tune the BERT model using Kiva's train dataset.\n",
        "7.   It's time to tell if a Kiva loan request will default by using the trained model.\n",
        "\n",
        "The experiment is best explained by the Colab notebook as follows:\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W6fEgxXIxkv1"
      },
      "source": [
        "Install TensorFlow 2.x:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "axpE7SL49ZVv"
      },
      "source": [
        "%tensorflow_version 2.x\n",
        "!pip3 install transformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MnpJWHbCyNYK"
      },
      "source": [
        "Download Kiva's train and test datasets:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BgCjTFQBnm5f"
      },
      "source": [
        "!wget -O kiva_train.csv https://drive.google.com/u/0/uc?id=1dzzVbgHphbCf7kvq9IKiIhwzmxPbuH4s&export=download\n",
        "!wget -O kiva_test.csv https://drive.google.com/u/0/uc?id=1EVWfyqQOd_W2uTKrr4JTD2iFrEZHoOHT&export=download"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6AHatzZkyc_R"
      },
      "source": [
        "Import TensorFlow and BERT tokenizer:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lmESy_wq9uCH"
      },
      "source": [
        "import tensorflow as tf\n",
        "from transformers import BertTokenizer\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-cased\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DUauBJpkyjZB"
      },
      "source": [
        "Load train and test datasets from CSV files:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zkxCi2CQWOCD"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "train_df = pd.read_csv (r'kiva_train.csv')\n",
        "test_df = pd.read_csv (r'kiva_test.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kzgUu-r8zPN8"
      },
      "source": [
        "Tokenize English texts:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "90tm-joeWbcg"
      },
      "source": [
        "tokenized_datasets = {\"train\": [], \"test\": []}\n",
        "\n",
        "for index, row in train_df.iterrows():\n",
        "    tokenized_row = tokenizer(row.en_clean, padding=\"max_length\", truncation=True)\n",
        "    tokenized_row[\"loan_id\"] = row.loan_id\n",
        "    tokenized_row[\"label\"] = row.defaulted\n",
        "    tokenized_datasets[\"train\"].append(tokenized_row)\n",
        "\n",
        "for index, row in test_df.iterrows():\n",
        "    tokenized_row = tokenizer(row.en_clean, padding=\"max_length\", truncation=True)\n",
        "    tokenized_datasets[\"test\"].append(tokenized_row)\n",
        "\n",
        "tokenized_datasets[\"train\"] = pd.DataFrame(tokenized_datasets[\"train\"])\n",
        "tokenized_datasets[\"eval\"] = tokenized_datasets[\"train\"].sample(frac=0.1, random_state=2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tkUP96niNfT8"
      },
      "source": [
        "Note: The following block should be removed when the model is used to predict loan defaults of the held-out test dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pOa_GKMWNaix"
      },
      "source": [
        "train_set_aside = []\n",
        "for index, row in tokenized_datasets[\"train\"].iterrows():\n",
        "    if row.loan_id not in list(tokenized_datasets[\"eval\"][\"loan_id\"]):\n",
        "        train_set_aside.append(row)\n",
        "tokenized_datasets[\"train\"] = pd.DataFrame(train_set_aside)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nsf6SpsLzdZL"
      },
      "source": [
        "Convert pandas dataframes to TensorFlow datasets:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OXaasHDPbZO5"
      },
      "source": [
        "train_features = {x: list(tokenized_datasets[\"train\"][x]) for x in tokenizer.model_input_names}\n",
        "train_tf_dataset = tf.data.Dataset.from_tensor_slices((train_features, list(tokenized_datasets[\"train\"][\"label\"])))\n",
        "train_tf_dataset = train_tf_dataset.shuffle(len(tokenized_datasets[\"train\"])).batch(16)\n",
        "\n",
        "eval_features = {x: list(tokenized_datasets[\"eval\"][x]) for x in tokenizer.model_input_names}\n",
        "eval_tf_dataset = tf.data.Dataset.from_tensor_slices((eval_features, list(tokenized_datasets[\"eval\"][\"label\"])))\n",
        "eval_tf_dataset = eval_tf_dataset.batch(16)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ughv7HGDzsMT"
      },
      "source": [
        "Load the pretrained BERT model from huggingface:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zub2HVehsGvb"
      },
      "source": [
        "from transformers import TFBertForSequenceClassification\n",
        "model = TFBertForSequenceClassification.from_pretrained(\"bert-base-cased\", num_labels=2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N50e-susz5i0"
      },
      "source": [
        "Fine tune the BERT model using Kiva's train dataset for three epochs and save the model: (Colab Pro usually stops the training after a day of execution. Better saving the model and then reloading it every few epochs. )"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Up-xPGu1lOjz"
      },
      "source": [
        "model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=5e-5),\n",
        "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    metrics=tf.metrics.SparseCategoricalAccuracy(),\n",
        ")\n",
        "\n",
        "\n",
        "checkpoint_filepath = 'bert-kiva-checkpoint'\n",
        "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_filepath,\n",
        "    save_weights_only=True,\n",
        "    monitor='val_sparse_categorical_accuracy',\n",
        "    mode='max',\n",
        "    save_best_only=True)\n",
        "\n",
        "model.fit(train_tf_dataset, validation_data=eval_tf_dataset, epochs=3,callbacks=[model_checkpoint_callback])\n",
        "model.save_pretrained(\"bert-kiva\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6yV5aj2-0cse"
      },
      "source": [
        "Reload the saved model from the previous step:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DhGrnxSSt6wN"
      },
      "source": [
        "from transformers import TFBertForSequenceClassification\n",
        "model = TFBertForSequenceClassification.from_pretrained(\"bert-kiva\", num_labels=2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BUkB2HAn2PyT"
      },
      "source": [
        "Train the BERT model for another three epochs:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sODNn68n-G2_"
      },
      "source": [
        "model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=5e-5),\n",
        "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    metrics=tf.metrics.SparseCategoricalAccuracy(),\n",
        ")\n",
        "\n",
        "\n",
        "checkpoint_filepath = 'bert-kiva-checkpoint'\n",
        "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_filepath,\n",
        "    save_weights_only=True,\n",
        "    monitor='val_sparse_categorical_accuracy',\n",
        "    mode='max',\n",
        "    save_best_only=True)\n",
        "\n",
        "model.fit(train_tf_dataset, validation_data=eval_tf_dataset, epochs=3,callbacks=[model_checkpoint_callback])\n",
        "model.save_pretrained(\"bert-kiva\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rGwcVr7H26-C"
      },
      "source": [
        "Predict loan defaults of the held-out test dataset:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Aiuc9ReBepHG"
      },
      "source": [
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "test_pred = []\n",
        "for row in tqdm(tokenized_datasets[\"test\"]):\n",
        "    row = dict(row)\n",
        "    row[\"input_ids\"] = tf.reshape(row[\"input_ids\"], (1,-1))\n",
        "    row[\"attention_mask\"] = tf.reshape(row[\"attention_mask\"], (1,-1))\n",
        "    row[\"token_type_ids\"] = tf.reshape(row[\"token_type_ids\"], (1,-1))\n",
        "    outputs = model(**row)\n",
        "    loss = outputs.loss\n",
        "    logits = outputs.logits\n",
        "    test_pred.append(np.argmax(logits))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XMebFMp73Q5i"
      },
      "source": [
        "Save the default results to a CSV file:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "svLFfL1geTed"
      },
      "source": [
        "test_df[\"defaulted\"] = test_pred\n",
        "test_df.to_csv(\"kiva_test_with_defaulted.csv\",index=False)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}